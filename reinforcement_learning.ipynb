{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference: https://dnddnjs.gitbooks.io/rl/content/\n",
    "\n",
    "# MDP; Markov Decision Process\n",
    "\n",
    "## 1. Markov Property\n",
    "- 미래의 State는 과거와 무관하게 현재의 State만으로 결정된다.\n",
    "$$\n",
    "P(s_{t+1}|s_{t}) = P(s_{t+1}|s_{t}, s_{t-1}, ... , s_{0})\n",
    "$$\n",
    "\n",
    "## 2. MP; Markov Process(= Markov Chain)\n",
    "- Time Interval이 discrete하고 Markov Property를 나타내는 확률 과정\n",
    "\n",
    "### (1) State Transition Matrix $P$ \n",
    "$$\n",
    "P_{ss'} \\overset{\\underset{\\mathrm{def}}{}}{=} P(S_{t+1}=s'|S_{t}=s)\n",
    "$$\n",
    "\n",
    "## 3. MRP; Markov Reward Process\n",
    "- MP에 Reward 개념을 추가한 확률 과정\n",
    "- MP에서는 각 state별 transition 확률이 주어져 있다할 뿐이지, 이 state에서 다음 state로 가는 것이 얼마나 가치가 있는지는 알 수 없습니다.\n",
    "\n",
    "### (1) Return $G_{t}$\n",
    "- 시점 t에서의 전체 미래에 대한 sum of discounted reward.\n",
    "$$\n",
    "G_{t} \\overset{\\underset{\\mathrm{def}}{}}{=} R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^\\infty \\gamma^{k}R_{t+k+1}\n",
    "$$\n",
    "\n",
    "### (2) State Value Function $V(s)$\n",
    "- Expectation of Return $G_{t}$\n",
    "$$\n",
    "V(s) \\overset{\\underset{\\mathrm{def}}{}}{=} E[G_{t}|S_{t}=s]\n",
    "$$\n",
    "\n",
    "## 4. MDP; Markov Decision Process\n",
    "- MRP에 Action 개념을 추가한 확률 과정\n",
    "- 이전에는 State가 Transition Probability에 따라 변했다면, 이제는 Action에 따라 변함.\n",
    "\n",
    "### (1) State Trainsition Matrix P\n",
    "$$\n",
    "P_{ss'}^{a} \\overset{\\underset{\\mathrm{def}}{}}{=} P(S_{t+1}=s'|S_{t}=s, A_{t}=a)\n",
    "$$\n",
    "\n",
    "### (2) Policy $\\pi$\n",
    "$$\\pi(a|s) \\overset{\\underset{\\mathrm{def}}{}}{=} P(A_{t}=a|S_{t}=s)$$\n",
    "\n",
    "$$P_{ss'}^{\\pi} = \\sum_{a \\in A} \\pi(a|s) P_{ss'}^a$$\n",
    "\n",
    "$$R_{s}^{\\pi} = \\sum_{a \\in A} \\pi(a|s) R_{s}^a$$\n",
    "\n",
    "### (3) State Value Function $V_{\\pi}(s)$\n",
    "- The expected return starting from state $s$, and then following policy $\\pi$\n",
    "$$\n",
    "V_{\\pi}(s) \\overset{\\underset{\\mathrm{def}}{}}{=} E_{\\pi}[G_{t}|S_{t}=s]\n",
    "$$\n",
    "\n",
    "#### Bellman (Expectation) Equation\n",
    "$$\n",
    "V_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma V(S_{t+1})|S_{t}=s]\n",
    "$$\n",
    "\n",
    "### (4) State-Action Value Function $Q_{\\pi}$\n",
    "- The expected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "$$\n",
    "Q_{\\pi}(s, a) \\overset{\\underset{\\mathrm{def}}{}}{=} E_{\\pi}[G_{t}|S_{t}=s, A_{t}=a]\n",
    "$$\n",
    "\n",
    "#### Bellman (Expectation) Equation\n",
    "$$\n",
    "Q_{\\pi}(s, a) = E_{\\pi}[R_{t+1} + \\gamma Q_{\\pi}(S_{t+1}, A_{t+1})|S_{t}=s, A_{t}=a]\n",
    "$$\n",
    "\n",
    "### (5) 관계식\n",
    "#### State-Action Value Function to State Value Function\n",
    "$$\n",
    "\\begin{align}\n",
    "V_{\\pi}(s) &= \\sum_{a \\in A} \\pi(a|s) Q_{\\pi}(s, a)\\\\\n",
    "&= \\sum_{a \\in A} \\pi(a|s) \\left( R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}V_{\\pi}(s') \\right)\\\\\n",
    "&= R_{s}^{\\pi} + \\gamma \\sum_{s' \\in S} P_{ss'}^{\\pi}V_{\\pi}(s')\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### State Value Function to State-Action Value Function\n",
    "$$\\begin{align}\n",
    "Q_{\\pi}(s, a) &= R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}V_{\\pi}(s')\\\\\n",
    "&= R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a} \\sum_{a' \\in A} \\pi(a'|s')Q_{\\pi}(s',a')\n",
    "\\end{align}$$\n",
    "\n",
    "### (6) Optimal Value Function\n",
    "- \"MDP를 풀었다\" = \"Optimal Value Function 혹은 그것을 만드는 $\\pi$를 찾았다\"\n",
    "#### Optimal State Value Function $V^{*}$\n",
    "$$\n",
    "V^{*}(s) = \\max_{\\pi} V_{\\pi}(s)\n",
    "$$\n",
    "#### Optimal State-Action Value Function $Q^{*}$\n",
    "$$\n",
    "Q^{*}(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "### (7) Optimal Policy Threom\n",
    "- 어떤 MDP에서도 다음이 성립한다.\n",
    "$$\n",
    "\\pi' \\ge \\pi \\leftrightarrow V_{\\pi'}(s) \\ge V_{\\pi}(s), \\forall s \\in S\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi^{*} \\ge \\pi, \\forall \\pi\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{\\pi^{*}}(s) = V^{*}(s),\\ Q_{\\pi^{*}}(s, a) = Q^{*}(s, a)\n",
    "$$\n",
    "- $Q^{*}$를 알고 있다면 Optimal Policy $\\pi^{*}$를 다음 방법으로 찾을 수 있다.\n",
    "$$\\pi^{*}(a|s)=\\left\\{\n",
    "\\begin{array}{c l}\t\n",
    "    1, & if\\ a = \\underset{{a \\in A}}{\\operatorname{argmax}}Q^{*}(s, a)\\\\\n",
    "    0, & otherwise\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "### (8) BOE; Bellman Optimality Equation\n",
    "- Linear Equation이 아니므로 일반해가 존재하지 않는다.\n",
    "$$\n",
    "V^{*}(s) = \\sum_{a \\in A} \\pi^{*}(a|s) Q^{*}(s, a) = \\max_{a \\in A}Q^{*}(s, a)\n",
    "$$\n",
    "$$\n",
    "Q^{*}(s, a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}V^{*}(s')\n",
    "$$\n",
    "$$\n",
    "V_{\\pi}(s) = \\max_{a \\in A} \\left( R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}V^{*}(s') \\right)\n",
    "$$\n",
    "$$\n",
    "Q_{\\pi}(s, a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\max_{a' \\in A}Q^{*}(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP; Dynamic Programming\n",
    "- Environment에 대해 알 때\n",
    "$$\n",
    "\\begin{align}\n",
    "V_{t+1}^{\\pi}(s) &= \\sum_{a \\in A} \\pi(a|s) \\left( R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{\\pi}V_{t}^{\\pi}(s') \\right)\\\\\n",
    "&= R_{s}^{\\pi} + \\gamma \\sum_{s' \\in S} P_{ss'}^{\\pi}V_{t}^{\\pi}(s')\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 1. Policy Iteration\n",
    "1. Policy $\\pi_{k}$에 대해 PE를 적용해 $V^{\\pi}$를 구한다.\n",
    "2. $\\pi_{k}$, $V^{\\pi}$에 대해 PI를 적용해 $\\pi_{k+1}$을 구한다.\n",
    "3. 위 과정을 $\\pi_{k+1} \\sim \\pi_{k}$일 때까지 반복한다. 이때 Optimal Policy $\\pi^{*} = \\pi_{k+1}$이다.\n",
    "\n",
    "### (1) PE; Policy Evaluation\n",
    "1. $V_{k}$에 Bellman Expectation Backup Operator $T^{\\pi}$ 적용해 $V_{k+1}$를 구한다.\n",
    "$$T^{\\pi}(V) \\overset{\\underset{\\mathrm{def}}{}}{=} R^{\\pi} + \\gamma P^{\\pi}V$$\n",
    "2. 위 과정을 $V_{k+1} \\sim V_{k}$ 때까지 반복한다. 이때 $V^{\\pi} = V_{k+1}$이다.\n",
    "\n",
    "### (2) PI; Policy Improvement\n",
    "1. 다음 식을 통해 $V^{\\pi}$로부터 $Q^{\\pi}$를 구한다.\n",
    "$$Q^{\\pi} = R + \\gamma PV^{\\pi}$$\n",
    "2. 다음 식을 통해 Improved Policy $\\pi_{k+1}$를 구한다.\n",
    "$$\\pi_{k+1} = \\left\\{\n",
    "\\begin{array}{c l}\t\n",
    "    1, & if\\ a = \\underset{{a \\in A}}{\\operatorname{argmax}}Q^{\\pi}\\\\\n",
    "    0, & otherwise\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "### (3) Policy Improvement Therom\n",
    "- PI를 통해 구한 $\\pi'$은 $\\pi' \\ge \\pi$를 만족한다.\n",
    "\n",
    "## 2. VI; Value Iteration\n",
    "1. $V_{k}$에 Bellman Optimal Backup을 적용해 $V_{k+1}$를 구한다.\n",
    "$$V_{k+1}(s) = \\max_{a \\in A}(R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{ss'}^{a}V_{k}(s'))$$\n",
    "$$\\bigg( V_{k+1} = \\max_{a \\in A}(R^{a} + \\gamma P^{a}V_{k}) \\bigg)$$\n",
    "2. 위 과정을 $V_{k+1} \\sim V_{k}$ 때까지 반복한다. 이때 $V^{*} = V_{k+1}$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free/On-policy Methods\n",
    "\n",
    "## 1. MC; Monte-Carlo Methods\n",
    "\n",
    "### (1) MC PE\n",
    "\n",
    "### (2) MC Control\n",
    "\n",
    "### (3) $\\epsilon$-Greedy PI\n",
    "$$\\pi(a|s) =\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "&\\frac{\\epsilon}{|A|} + 1 - \\epsilon\n",
    "&&if\\ a = \\underset{a \\in A}{\\operatorname{argmax}}Q^{\\pi}(s, a)\\\\\n",
    "&\\frac{\\epsilon}{|A|}\n",
    "&&otherwise\n",
    "\\end{align}\n",
    "\\right.$$\n",
    "\n",
    "#### GLIE; Greedy in the Limit with Infinite Exploration\n",
    "- If all state-action pairs are explored infinitely many times($N(s, a) \\rightarrow \\infty$), the policy converges on a greedy policy.\n",
    "\n",
    "## 2. TD; Temporal Difference Methods\n",
    "\n",
    "### (1) TD PE\n",
    "\n",
    "### (2) SARSA PE\n",
    "- Initialize $Q(s, a)$ arbitrarily $\\forall s \\in S,\\ a \\in A$.\n",
    "- (For each episode) Repeat:\n",
    "    - Choose action $a$ from state $s$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)\n",
    "    - (For each step of episode) Repeat:\n",
    "        - Take action $a$, observe return $r$, state $s'$\n",
    "        - Choose action $a'$ from state $s'$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)\n",
    "        - $Q(s, a) \\leftarrow Q(s, a) + \\alpha(r + \\gamma Q(s', a') - Q(s, a))$\n",
    "        - $s \\leftarrow s',\\ a \\leftarrow a'$\n",
    "    - Stop if state $s$ is terminal.\n",
    "    \n",
    "### (3) TD Control(= SARSA Control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Methods\n",
    "- 임의로 정한 $\\mu$으로 구한 episode에서도 $Q$를 추산\n",
    "- Evaluate target policy $\\pi(a|s)$ to compute $V^{\\pi}(s)$, $Q^{\\pi}(s, a)$ while following behavior policy $\\mu(a|s)$.\n",
    "- Learng from observing humans or ohter agents.\n",
    "- Re-use experience generated from old policies $\\pi_{1},\\ \\pi_{2}, ...$\n",
    "- Learn about optimal policy while following exploratory policy.\n",
    "- Learn about multiple policies while following one pilicy.\n",
    "- MDP Model을 몰라도 계산 가능하다.\n",
    "\n",
    "## 1. Importance Sampling\n",
    "- Estimate the expectation of a different distribution.\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{x \\sim P}[f(x)]\n",
    "&= \\sum_{x \\in X}p(x)f(x)\\\\\n",
    "&= \\sum_{x \\in X}q(x)\\frac{p(x)}{q(x)}f(x)\\\\\n",
    "&= \\mathbb{E}_{x \\sim Q}\\left[\\frac{p(x)}{q(x)}f(x)\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "### (1) Importance Sampling for Off-Policy MC\n",
    "- Use returns generated from $\\mu$ to evaluate $\\pi$\n",
    "- Weight return $G_{t}$ according to similarity between policies.\n",
    "- Multiply importance sampling corrections along whole episode.\n",
    "$$G_{t}^{\\pi/\\mu} = \\prod_{k=t}^{T-1}\\frac{\\pi(A_{k}|S_{k})}{\\mu(A_{k}|S_{k})}G_{t}$$\n",
    "- Update value towards corrected return.\n",
    "$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha(G_{t}^{\\pi/\\mu} - V(S_{t}))$$.\n",
    "- Cannot use if $\\mu=0$ when $\\pi \\ne 0$.\n",
    "- Can dramatically increase variance.\n",
    "- $p(s)$는 함수 형태는 알지만 샘플을 만들기 어려울 때 $q(s)$라는 샘플링하기 쉬운 분포를 활용.\n",
    "\n",
    "### (2) Importance Sampling for Off-Policy TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation\n",
    "- Adam(Adaptive Momentum Estimation) = Momentum SGD + RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\pi$의 shape: `[n_states, n_acts]`\n",
    "- $R$의 shape: `[n_states, n_acts]`\n",
    "- $P$의 shape: `[n_states, n_states, n_acts]`\n",
    "- $R^{\\pi}$의 shape: `[n_states, 1]`\n",
    "- $P^{\\pi}$의 shape: `[n_states, n_states]`"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212.882px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
